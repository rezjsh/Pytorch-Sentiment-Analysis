# --- configs/config.yaml ---

project_name: "Sentiment_Project"

data:
  # Hugging Face dataset name
  dataset_name: "imdb"
  # Maximum sequence length for all tokenizers (DL models)
  max_length: 128
  # Global batch size used by data loaders
  batch_size: 32
  # Ratio for test set split (e.g., 0.1 means 10% for test)
  test_split_ratio: 0.1
  # Random seed for reproducibility across splits and models
  seed: 42
  # Tokenizer names for different models
  tokenizer_name: "bert-base-uncased"

eda:
  report_dir: "reports/eda_reports"
  max_words_to_plot: 50

model:
  model_type: "SBERT"

callacks:
  model_checkpoint:
    checkpoint_dir: "checkpoints/"

trainer:
  report_dir: "reports/model_reports"

evaluation:
  model_path: "checkpoints/best_model.pth"
  report_dir: "reports/evaluation_reports"

trainer_defaults:
  # Maximum epochs to run (Early Stopping will stop it sooner)
  max_epochs: 15
  # Frequency for logging steps during training (e.g., to TensorBoard/stdout)
  log_every_n_steps: 10

paths:
  # Directory where model checkpoints are saved
  checkpoints_dir: "checkpoints"
  # Directory where logs (e.g., TensorBoard) are stored
  logs_dir: "tb_logs"

# The name of the model currently selected for execution
current_model: "SBERT"
