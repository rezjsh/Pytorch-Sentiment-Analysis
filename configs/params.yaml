# --- configs/params.yaml ---
eda:
  max_words_to_plot: 50
  min_word_len: 3

ml_baseline:
  classifier_name: "LOGREG" # Options: LOGREG, SVM
  max_features: 10000 # Size of TF-IDF vocabulary
  ngram_range: [1, 2] # [min_n, max_n] e.g., Unigrams and Bigrams
  C: 1.0 # Regularization strength
  max_iter: 2000 # Maximum iterations for model convergence

# --- Model Specific Hyperparameters ---
model_options:
  # Classic Deep Learning Models
  LSTM:
    embedding_dim: 100
    hidden_size: 256
    num_layers: 2
    dropout: 0.5
    bidirectional: True
    batch_first: True

  CNN:
    embedding_dim: 100
    num_filters: 100
    filter_sizes: [3, 4, 5]
    dropout: 0.5

  # Advanced Deep Learning Models
  BILSTMATTENTION:
    embedding_dim: 100
    hidden_size: 256
    num_layers: 2
    dropout: 0.5

  # Modern Transfer Learning Models
  BERT:
    dropout: 0.1
    # Note: 'distilbert-base-uncased' is the model ID from Hugging Face
    model_name: "distilbert-base-uncased"

  SBERT:
    dropout: 0.1
    # Note: 'sentence-transformers/all-MiniLM-L6-v2' is the model ID
    model_name: "sentence-transformers/all-MiniLM-L6-v2"

callbacks:
  # Early Stopping Configuration
  early_stopping:
    patience: 5
    mode: "min"
    min_delta: 0.001

  # Learning Rate Scheduler Configuration
  lr_scheduler:
    patience: 2
    factor: 0.5
    mode: "min"

  # Gradient Clipping Configuration
  gradient_clip:
    max_norm: 1.0

# --- Global Training Parameters ---
trainer:
  # Optimizer learning rate for Deep Learning models
  learning_rate: 0.0001
  max_epochs: 20
  # Callbacks Configuration
  gradient_clip_norm: 1.0 # Maximum L2 norm for gradient clipping
  es_patience: 5 # Early Stopping: wait this many epochs before stopping
  lr_scheduler_patience: 2 # LR Scheduler: wait this many epochs before reducing LR
  lr_scheduler_factor: 0.5 # LR Scheduler: factor by which the LR will be reduced (new_lr = old_lr * factor)
